# servier-test

## Description

This project is a data pipeline that processes information about drugs, clinical trials, and scientific publications. It generates a JSON file representing a graph of connections between drugs and their mentions in publications. Additionally, it includes a feature to identify the journal that mentions the most different drugs.

## Project Structure

- `dags/`: Contains the DAG files for Apache Airflow.
- `servier/src/`: Contains the source code for the data pipeline.
- `data/`: Directory for storing data generated by the pipeline.
- `Makefile`: Commands to facilitate the execution of tasks.
- `sql/`: Contains the SQL queries for the second skill test.

## Output Format

```json
{
  "drugs": {
    "A04AD": {
      "pubmed_mentions": [
        {
          "id": 1,
          "title": "A 44-year-old man with erythema of the face diphenhydramine, neck, and chest, weakness, and palpitations",
          "journal": "Journal of emergency nursing",
          "date": "2019-01-01",
          "drug": "DIPHENHYDRAMINE"
        },
        {
          "id": 2,
          "title": "An evaluation of benadryl, pyribenzamine, and other so-called diphenhydramine antihistaminic drugs in the treatment of allergy.",
          "journal": "Journal of emergency nursing",
          "date": "2019-01-01",
          "drug": "DIPHENHYDRAMINE"
        },
        {
          "id": 3,
          "title": "Diphenhydramine hydrochloride helps symptoms of ciguatera fish poisoning.",
          "journal": "The Journal of pediatrics",
          "date": "2019-01-02",
          "drug": "DIPHENHYDRAMINE"
        }
      ],
      "clinical_trials": [
        {
          "id": "NCT01967433",
          "title": "Use of Diphenhydramine as an Adjunctive Sedative for Colonoscopy in Patients Chronically on Opioids",
          "journal": "Journal of emergency nursing",
          "date": "2020-01-01",
          "drug": "DIPHENHYDRAMINE"
        },
        {
          "id": "NCT04189588",
          "title": "Phase 2 Study IV QUZYTTIR (Cetirizine Hydrochloride Injection) vs V Diphenhydramine",
          "journal": "Journal of emergency nursing",
          "date": "2020-01-01",
          "drug": "DIPHENHYDRAMINE"
        },
        {
          "id": "NCT04237091",
          "title": "Feasibility of a Randomized Controlled Clinical Trial Comparing the Use of Cetirizine to Replace Diphenhydramine in the Prevention of Reactions Related to Paclitaxel",
          "journal": "Journal of emergency nursing",
          "date": "2020-01-01",
          "drug": "DIPHENHYDRAMINE"
        }
      ]
    },
    "S03AA": {
      "pubmed_mentions": [
        {
          "id": 4,
          "title": "Tetracycline Resistance Patterns of Lactobacillus buchneri Group Strains.",
          "journal": "Journal of food protection",
          "date": "2020-01-01",
          "drug": "TETRACYCLINE"
        },
        {
          "id": 5,
          "title": "Appositional Tetracycline bone formation rates in the Beagle.",
          "journal": "American journal of veterinary research",
          "date": "2020-01-02",
          "drug": "TETRACYCLINE"
        },
        {
          "id": 6,
          "title": "Rapid reacquisition of contextual fear following extinction in mice: effects of amount of extinction, tetracycline acute ethanol withdrawal, and ethanol intoxication.",
          "journal": "Psychopharmacology",
          "date": "2020-01-01",
          "drug": "TETRACYCLINE"
        }
      ],
      "clinical_trials": [
        {
          "id": "NCT01967434",
          "title": "Study on Tetracycline in the Treatment of Acne",
          "journal": "Journal of clinical dermatology",
          "date": "2021-02-01",
          "drug": "TETRACYCLINE"
        }
      ]
      ...
    }
  }
}
```

In designing the output JSON structure for my data pipeline, I chose a nested format to represent the connections between drugs and their mentions in various publications. Here’s why I believe this structure is particularly relevant:

- **Logical Grouping**: by using a nested structure, I can logically group related information together. Each drug is represented as a key, with its associated mentions (both from PubMed and clinical trials) organized under that key. This makes it intuitive for me to access all relevant information about a specific drug in one place.

- **Hierarchical Representation**: the nested format allows me to represent complex relationships in a clear and organized manner. For example, all mentions of a specific drug are contained within its own object, which simplifies data retrieval and enhances readability.

- **Ease of Access**: when I query or process the data, the nested structure enables straightforward access patterns. If I want to retrieve all mentions for a specific drug, I can directly access that drug's object and its associated mentions without needing to filter through a flat list.

- **Reduced Redundancy**: this structure minimizes redundancy by allowing related data to be stored together. Instead of repeating drug identifiers for each mention, the drug's information is encapsulated within its own object, which reduces the overall size of the data and improves efficiency.

- **Scalability**: as the dataset grows, I can easily accommodate additional layers of information within the nested structure without complicating the overall design. For instance, if I later decide to include more details about each mention (like author names or publication types), I can add these attributes within the existing structure seamlessly.

- **Contextual Relevance**: The nested structure provides context for each mention. When I look at a specific drug, I can see both its PubMed mentions and clinical trial data together, which is crucial for understanding the drug's relevance in different contexts.

## Running Apache Airflow with Docker

### Prerequisites

The project requires Docker and Docker Compose installed on your machine.

Before installation, you can run the following commands locally to see the output:

```bash
make get_output # To retrieve the generated json file
make find_top_journal # To find the journal that mentions the most different drugs
```

### Installation

**Start the services**:

   ```bash
   make init_docker
   docker-compose up -d
   ```

## Summary of commands

Here is a summary of the commands you can execute:

```bash
make clean
make init_docker # To initialize and start the services
make run_manual_dag # To manually trigger the Airflow pipeline
make get_output # To retrieve the generated json file
make find_top_journal # To find the journal that mentions the most different drugs
```

### Explanation of `make find_top_journal`

The command `make find_top_journal` executes a Python script that analyzes the output JSON file (`data/drug_mentions_graph.json`) and identifies the journal that mentions the most different drugs. This provides valuable insights into the publications and their coverage of drugs, which can be useful for further analysis or strategic decisions.

## Next Steps

Among other things, here are the very next things to do:

1. **Unit Testing**: Implement unit tests for the functions in the pipeline to ensure they work as expected and to catch any potential bugs early.

2. **Function Testing**: Conduct function testing to validate the overall functionality of the pipeline, ensuring that all components work together seamlessly.

3. **CI/CD**: Set up Continuous Integration and Continuous Deployment (CI/CD) pipelines to automate testing and deployment processes, improving the efficiency and reliability of updates.

4. **Build Versioned Docker Image**: Create a versioned Docker image that includes the Python package, allowing for easier management of different versions of the application.

5. **Update Airflow DAG on Server**: This involves deploying the updated DAG files to the Airflow server, ensuring that the latest changes and features are available for execution. This may include updating the DAG definitions, adding new tasks, or modifying existing ones to reflect the latest logic in the data pipeline.

## For Further Development

To evolve my code to handle large volumes of data (files of several terabytes or millions of files), I consider several key factors:

### Scalability
- I prefer using distributed architectures like Apache Spark for processing massive data in parallel. While traditional multiprocessing and multithreading techniques can be effective for smaller datasets, they have limitations when it comes to scalability. Here’s why I lean towards Spark in this use case:

- **Distributed Processing**: Spark is designed to distribute workloads across multiple nodes in a cluster. This means that as my data grows, I can simply add more nodes to the cluster to handle the increased load. In contrast, multiprocessing and multithreading are typically limited to a single machine, which can become a bottleneck when dealing with large datasets.

- **Fault Tolerance**: Spark provides built-in fault tolerance. If a node fails during processing, the system can recover and continue without losing progress. This is crucial when working with large volumes of data, as failures can be more common in extensive processing tasks. Traditional multiprocessing does not inherently offer this level of fault tolerance.

- **Optimized for Big Data**: Spark, in particular, is optimized for big data processing. This flexibility allows me to work with large datasets more efficiently than I could with standard multiprocessing techniques.

- **Ease of Use**: With high-level APIs, Spark allows me to write less code to achieve complex data processing tasks. This can significantly speed up development time and reduce the likelihood of bugs compared to managing threads or processes manually.

### Storage

- I also recognize the importance of transitioning to distributed storage systems like Amazon S3 and Google Cloud Storage. These systems are designed to efficiently store and retrieve large volumes of data, making them ideal for my needs as the dataset grows.

### Query Optimization

- For querying large datasets, I plan to use databases optimized for massive queries, such as Amazon Redshift or Google BigQuery. These databases are specifically designed to handle large-scale data analytics quickly and efficiently.

### Error Handling

- Implementing robust error handling mechanisms is essential for managing processing failures, especially when dealing with large data volumes. I will ensure that my pipeline can gracefully handle errors and continue processing where possible.

### Monitoring and Alerts

- Setting up monitoring systems to track the performance of my pipeline is crucial. I want to receive alerts in case of issues, which will help maintain reliability when processing large volumes of data.

### Performance Testing

- Finally, I will conduct performance testing to identify bottlenecks in my pipeline and optimize the slowest parts. This proactive approach will help ensure that my pipeline remains efficient as data volumes increase.

By integrating these elements, I believe my pipeline will be better prepared to handle large data volumes and evolve with future needs. The combination of distributed processing, efficient storage, and robust error handling will provide a solid foundation for scaling my data processing capabilities.

## Notes
- This project was completed in less than half a day due to personal constraints.
- I noticed data issues in the output of my pipeline, including IDs that can be empty (`""`) from the PubMed source, as well as encoding/decoding errors (e.g., `H\u00f4pitaux Universitaires de Gen\u00e8ve`).
- I encountered some issues running Apache Airflow with Docker/Docker Compose, which may lead to problems.
- With more time, I could have done things better.